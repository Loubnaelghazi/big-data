{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb184a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql.functions import col, when\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589c1841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+---------+--------------------+\n",
      "|Tweet ID|     Entity|Sentiment|       Tweet content|\n",
      "+--------+-----------+---------+--------------------+\n",
      "|    2401|Borderlands| Positive|I am coming to th...|\n",
      "|    2401|Borderlands| Positive|im getting on bor...|\n",
      "|    2401|Borderlands| Positive|im coming on bord...|\n",
      "|    2401|Borderlands| Positive|im getting on bor...|\n",
      "|    2401|Borderlands| Positive|im getting into b...|\n",
      "|    2402|Borderlands| Positive|So I spent a few ...|\n",
      "|    2402|Borderlands| Positive|So I spent a coup...|\n",
      "|    2402|Borderlands| Positive|So I spent a few ...|\n",
      "|    2402|Borderlands| Positive|So I spent a few ...|\n",
      "|    2402|Borderlands| Positive|2010 So I spent a...|\n",
      "|    2402|Borderlands| Positive|                 was|\n",
      "|    2403|Borderlands|  Neutral|Rock-Hard La Varl...|\n",
      "|    2403|Borderlands|  Neutral|Rock-Hard La Varl...|\n",
      "|    2403|Borderlands|  Neutral|Rock-Hard La Varl...|\n",
      "|    2403|Borderlands|  Neutral|Rock-Hard La Vita...|\n",
      "|    2403|Borderlands|  Neutral|Live Rock - Hard ...|\n",
      "|    2403|Borderlands|  Neutral|I-Hard like me, R...|\n",
      "|    2404|Borderlands| Positive|that was the firs...|\n",
      "|    2404|Borderlands| Positive|this was the firs...|\n",
      "|    2404|Borderlands| Positive|that was the firs...|\n",
      "+--------+-----------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Tweet ID\", IntegerType(), True),\n",
    "    StructField(\"Entity\", StringType(), True),\n",
    "    StructField(\"Sentiment\", StringType(), True),\n",
    "    StructField(\"Tweet content\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(\"twitter_training.csv\", header=True, schema=schema)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c817cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.withColumn('Tweet content', when(col('Tweet content').isNull(),'').otherwise(col('Tweet content')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "222968a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"Tweet content\", outputCol=\"words\")\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\")\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "indexer = StringIndexer(inputCol=\"Sentiment\", outputCol=\"label\",handleInvalid=\"skip\")\n",
    "assembler = VectorAssembler(inputCols=[\"features\"], outputCol=\"final_features\")\n",
    "\n",
    "lr = LogisticRegression(featuresCol='final_features', labelCol='label')\n",
    "\n",
    "df_cleaned= indexer.fit(df_cleaned).transform(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cd3f15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer, stopwords_remover, hashing_tf, idf, assembler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a1c83cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df_cleaned.randomSplit([0.8, 0.2], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dd997cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9402b5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ca70a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"accuracy\")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"f1\")\n",
    "evaluator_recall = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"weightedRecall\")\n",
    "\n",
    "accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "f1_score = evaluator_f1.evaluate(predictions)\n",
    "recall = evaluator_recall.evaluate(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc5bab69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy :  0.8257951952758019\n",
      "f1_score 0.8258296978404729\n",
      "recall 0.8257951952758019\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy : \" ,accuracy)\n",
    "print(\"f1_score\" , f1_score)\n",
    "print(\"recall\" , recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f85ffe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model.save('pipeline_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227fa89c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec41c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88e5706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
