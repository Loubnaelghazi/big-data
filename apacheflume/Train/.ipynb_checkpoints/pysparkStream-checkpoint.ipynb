{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "589c1841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|      \"File \\u001...|\n",
      "|      \"File \\u001...|\n",
      "|      \"File \\u001...|\n",
      "|      \"File \\u001...|\n",
      "|      \"traceback\": [|\n",
      "|      \"\\u001b[1;3...|\n",
      "|      \"\\u001b[1;3...|\n",
      "|      \"Cell \\u001...|\n",
      "|      \"File \\u001...|\n",
      "|      \"Cell \\u001...|\n",
      "|      \"File \\u001...|\n",
      "|10951,TomClancysG...|\n",
      "|10951,TomClancysG...|\n",
      "|10951,TomClancysG...|\n",
      "|10952,TomClancysG...|\n",
      "|10952,TomClancysG...|\n",
      "|10952,TomClancysG...|\n",
      "|3333,Facebook,Irr...|\n",
      "|3334,Facebook,Irr...|\n",
      "|3334,Facebook,Irr...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read from HDFS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = spark.read.text(\"hdfs://localhost:9000/user/Hadoop/twit2/*\")\n",
    "data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c817cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|10951,TomClancysG...|\n",
      "|10951,TomClancysG...|\n",
      "|10951,TomClancysG...|\n",
      "|10952,TomClancysG...|\n",
      "|10952,TomClancysG...|\n",
      "|10952,TomClancysG...|\n",
      "|3333,Facebook,Irr...|\n",
      "|3334,Facebook,Irr...|\n",
      "|3334,Facebook,Irr...|\n",
      "|3334,Facebook,Irr...|\n",
      "|3334,Facebook,Irr...|\n",
      "|3334,Facebook,Irr...|\n",
      "|3334,Facebook,Irr...|\n",
      "|3335,Facebook,Irr...|\n",
      "|3335,Facebook,Irr...|\n",
      "|3335,Facebook,Irr...|\n",
      "|286,Amazon,Neutra...|\n",
      "|286,Amazon,Neutra...|\n",
      "|286,Amazon,Neutra...|\n",
      "|286,Amazon,Neutra...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "filtered_data = data.filter(~data[\"value\"].startswith('   ') )\n",
    "filtered_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "222968a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "split_col = split(filtered_data['value'], ',')\n",
    "\n",
    "filtered_data = filtered_data.withColumn('id', split_col.getItem(0)) \\\n",
    "                             .withColumn('topic', split_col.getItem(1)) \\\n",
    "                             .withColumn('sentiment', split_col.getItem(2)) \\\n",
    "                             .withColumn('content', split_col.getItem(3))\n",
    "\n",
    "filtered_data = filtered_data.drop('value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cd3f15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+--------------------+\n",
      "|   id|               topic| sentiment|             content|\n",
      "+-----+--------------------+----------+--------------------+\n",
      "|10951|TomClancysGhostRecon|  Negative|@AskPS_UK  @Ubiso...|\n",
      "|10951|TomClancysGhostRecon|  Negative|@AskPS_UK 3 @Ubis...|\n",
      "|10951|TomClancysGhostRecon|  Negative|@AskPS_UK @Ubisof...|\n",
      "|10952|TomClancysGhostRecon|   Neutral|The Terminator ev...|\n",
      "|10952|TomClancysGhostRecon|   Neutral|The Terminator ev...|\n",
      "|10952|TomClancysGhostRecon|   Neutral|\"The event dedica...|\n",
      "| 3333|            Facebook|Irrelevant|Creepy geek not o...|\n",
      "| 3334|            Facebook|Irrelevant|Halloween died th...|\n",
      "| 3334|            Facebook|Irrelevant|\"Halloween died t...|\n",
      "| 3334|            Facebook|Irrelevant|\"Halloween died t...|\n",
      "| 3334|            Facebook|Irrelevant|Halloween died th...|\n",
      "| 3334|            Facebook|Irrelevant|Halloween died th...|\n",
      "| 3334|            Facebook|Irrelevant|Halloween died th...|\n",
      "| 3335|            Facebook|Irrelevant|               The  |\n",
      "| 3335|            Facebook|Irrelevant|                    |\n",
      "| 3335|            Facebook|Irrelevant|              \"Italy|\n",
      "|  286|              Amazon|   Neutral|amazon.in / Shiva...|\n",
      "|  286|              Amazon|   Neutral|amazon.in/Shivaji...|\n",
      "|  286|              Amazon|   Neutral|Mon amazon. in / ...|\n",
      "|  286|              Amazon|   Neutral|amazon.in/Shivaji...|\n",
      "+-----+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30a72c1",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a1c83cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, StringIndexer, NGram\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes, LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "805106d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = filtered_data.na.drop()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39f5268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the 'content' column\n",
    "tokenizer = Tokenizer().setInputCol('content').setOutputCol('words')\n",
    "\n",
    "# Remove stopwords\n",
    "stopwords = StopWordsRemover().getStopWords() + ['-']\n",
    "remover = StopWordsRemover().setStopWords(stopwords).setInputCol('words').setOutputCol('filtered')\n",
    "\n",
    "# Create bigrams\n",
    "bigram = NGram().setN(2).setInputCol('filtered').setOutputCol('bigrams')\n",
    "\n",
    "# Generate features using CountVectorizer\n",
    "cvmodel = CountVectorizer().setInputCol('filtered').setOutputCol('features')\n",
    "cvmodel_ngram = CountVectorizer().setInputCol('bigrams').setOutputCol('features')\n",
    "\n",
    "# Convert 'sentiment' column to a binary label\n",
    "indexer = StringIndexer().setInputCol('sentiment').setOutputCol('label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "191d4719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|   id|               topic|sentiment|             content|               words|            filtered|             bigrams|            features|label|\n",
      "+-----+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|10951|TomClancysGhostRecon| Negative|@AskPS_UK  @Ubiso...|[@askps_uk, , @ub...|[@askps_uk, , @ub...|[@askps_uk ,  @ub...|(57034,[3,66,168,...|  0.0|\n",
      "|10951|TomClancysGhostRecon| Negative|@AskPS_UK 3 @Ubis...|[@askps_uk, 3, @u...|[@askps_uk, 3, @u...|[@askps_uk 3, 3 @...|(57034,[29,66,110...|  0.0|\n",
      "|10951|TomClancysGhostRecon| Negative|@AskPS_UK @Ubisof...|[@askps_uk, @ubis...|[@askps_uk, @ubis...|[@askps_uk @ubiso...|(57034,[66,168,63...|  0.0|\n",
      "|10952|TomClancysGhostRecon|  Neutral|The Terminator ev...|[the, terminator,...| [terminator, event]|  [terminator event]|(57034,[377,2274]...|  2.0|\n",
      "|10952|TomClancysGhostRecon|  Neutral|The Terminator ev...|[the, terminator,...| [terminator, event]|  [terminator event]|(57034,[377,2274]...|  2.0|\n",
      "+-----+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define the pipeline stages\n",
    "pipeline_preprocess = Pipeline(stages=[tokenizer, remover, bigram, cvmodel, indexer])\n",
    "\n",
    "# Fit the pipeline to your data DataFrame\n",
    "preprocessed_model = pipeline_preprocess.fit(filtered_data)\n",
    "\n",
    "# Transform the data DataFrame\n",
    "preprocessed_data = preprocessed_model.transform(filtered_data)\n",
    "\n",
    "# Show the first 5 rows of the preprocessed DataFrame\n",
    "preprocessed_data.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e45c808",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aceecf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData, testData  = filtered_data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10f201cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "pipeline_nb = Pipeline(stages=[tokenizer, remover, cvmodel, indexer, nb])\n",
    "model_nb = pipeline_nb.fit(trainingData)\n",
    "predictions_nb = model_nb.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f599d044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.735803355162824\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions_nb)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad522e63",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot pickle '_thread.RLock' object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnaive_bayes_model.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 5\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(model_nb, f)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot pickle '_thread.RLock' object"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the trained model\n",
    "with open(\"naive_bayes_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_nb, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c0e77a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb.write().overwrite().save('C:/Users/hanane/Desktop/BigDataProje/apacheflume/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f030d812",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "226b486a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3017811930884378\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "pipeline = Pipeline(stages = [tokenizer, remover, cvmodel, indexer, log_reg])\n",
    "model = pipeline.fit(trainingData)\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "AUC = evaluator.evaluate(predictions)\n",
    "print(AUC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2371108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33940389996890685\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import  RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier().setLabelCol('label').setFeaturesCol('features').setNumTrees(10)\n",
    "pipeline = Pipeline(stages = [tokenizer, remover, cvmodel, indexer, rf])\n",
    "model = pipeline.fit(trainingData)\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "AUC = evaluator.evaluate(predictions)\n",
    "print(AUC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd997cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
